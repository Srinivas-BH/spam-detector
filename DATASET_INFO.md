# Dataset Information

## Important Note
Large dataset files are excluded from this repository due to GitHub's 100MB file size limit.

## Required Datasets

To use this spam detection system, you'll need to obtain the following datasets:

### 1. Main Training Dataset
- **File**: `spam_data.csv` (or `master_spam_dataset.csv`)
- **Format**: CSV with columns `Category` (ham/spam) and `Message` (text)
- **Size**: ~135 MB
- **Source**: Available from various spam detection datasets on Kaggle

### 2. Additional Datasets (Optional)
- `ham_spam_emails.csv` - Email dataset (285 MB)
- `new_spam_data.csv` - Additional spam samples
- `email_phishing_data.csv` - Phishing email features

## Creating Your Dataset

1. **Option A - Use existing small dataset:**
   - The repository may include a smaller sample dataset
   - Use it directly for testing

2. **Option B - Merge multiple datasets:**
   ```bash
   python create_master_dataset.py
   ```
   This script will:
   - Load all available CSV files
   - Merge them into `master_spam_dataset.csv`
   - Remove duplicates
   - Balance the dataset

3. **Option C - Download from Kaggle:**
   - Search for "spam email dataset" on Kaggle
   - Download a dataset with CSV format
   - Ensure it has columns for message text and label/category

## Dataset Format Requirements

Your dataset CSV file should have:
- Column 1: `Category` or `label` (values: "ham"/"spam" or 0/1)
- Column 2: `Message` or `text` (the email/message text)

Example:
```csv
Category,Message
ham,"Hey, how are you?"
spam,"URGENT! Claim your prize now!"
```

## Training with Your Dataset

1. Place your dataset file as `spam_data.csv` or update the path in `train_ensemble_model.py`
2. Run the training script:
   ```bash
   python train_ensemble_model.py
   ```
3. The script will automatically:
   - Load the dataset
   - Preprocess the text
   - Train multiple models
   - Save the trained models

## Storage Recommendations

For large datasets (>100MB):
- Use Git LFS (Git Large File Storage)
- Store on cloud storage (Google Drive, Dropbox) with a link
- Use a data hosting service
- Keep locally and document how to obtain the dataset

## Model Files

Trained model files (`.pkl`) are also excluded from git because:
- They are large (50-100 MB each)
- They can be regenerated by running the training script
- They are machine-specific

To generate model files:
```bash
python train_ensemble_model.py
```

This will create:
- `spam_model_ensemble.pkl` - Main ensemble model
- `vectorizer.pkl` - TF-IDF vectorizer
- `feature_info.pkl` - Feature metadata
- `model_rf.pkl` - Random Forest model
- `model_nb.pkl` - Naive Bayes model
- `model_lr.pkl` - Logistic Regression model
- `model_gb.pkl` - Gradient Boosting model

